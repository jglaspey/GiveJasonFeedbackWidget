I want to build an AI dev harness for building AI agents for business processes. My thought is that it would be in a parent folder above the project I'm building. That way I could have an instance of Claude Code running in that directory that would have the relevant .claude files with my CLAUDE.md suited for building the AI agents. I could have subagents for tasks like committing to github and tracking my time with descriptions of the work and things like that. Then, when I testing and running my business process, I can load another instance of Claude Code that is in my project folder and it has it's own .claude and mcp.json files that are specific to running that business process.

I would want my CLAUDE.md file to be something like:
'/Users/justinkistner/Documents/GitHub/agent_dev_harness/claude_documentation/community_insights/obra_claude_dotfile.md' I would imagine it needs adaptation for my niche building purpose, but there's a lot I would keep as well since it applies to good software building practices generally.

I would want my dev harness to have skills
'/Users/justinkistner/Documents/GitHub/agent_dev_harness/claude_documentation/claude_skills' I would want these skills to not only know the technical implementation details for Claude Code and Agent SDK features, but also the knowledge of when to use them and for what purpose and how to configure them properly for specific desired outcomes.

For example, I did my own learning exercise to better understand settings.json files and how configuring them actually impacts the permission experience. I learned that the validator will accept: "Bash(test :*)", but that it won't work with a space before the colon. It needs to be: "Bash(test:*)". I also learned that you can't use wildcards in file names, only directories. Stuff like that. They way I did it was I had two instances of Claude Code open. One was the dev and the other the tester. With the dev instance, we made a testing plan for things we wanted to learn. Then, we made changes to the settings.json file. After restarting the tester instance in order to inherit the new settings, I would tell the tester instance to run a test and then record the results on the test plan doc. Then, I would tell the dev instance to refer to the doc and update our test plans based on what we learned.

Examples of things I want to learn:
- How do skills actually impact the experience when configured according to the documentation? I've heard people say they aren't always invoked when they expect them to be, so do I need to combine it with hooks or make better descriptions in the YAML? Are there best practices? Do those vary depending on the application?
- How do I make custom tools? What are good use cases for them? Anything not obvious about getting them to work?
- Can I get Claude Code to operate another instance of Claude Code to do the testing like I did for settings.json file in order to do the process autonomously?
- I'm also curious if I can do an approach like this: https://simonwillison.net/2025/Nov/6/async-code-research/ in order to more deeply understand how to weild the Claude Code and Agent SDK features. Again, similar to the process I did for better understand the settings.json file. Here's an example of a skill to learn skills, so maybe there's something useful in there about how we might approach this as well:
'/Users/justinkistner/Documents/GitHub/agent_dev_harness/claude_documentation/community_insights/writing_skills_tdd_approach.md'

More examples of skills I want:
- This is a good example of rules for a git subagent:
'/Users/justinkistner/Documents/GitHub/agent_dev_harness/claude_documentation/community_insights/steipete_agent_rules.md'
- I would want skills that are about good project structure and building hygene. For example, if we're testing something we should
write test scripts in test_scripts folder, so they don't end up strewn all over our project folder. If we're running an operation
that iterates over a long list, we should make the python script so it can use python workers. We should always use log files for times we iterate over lists so we have resumability. We should store errors we get into an errors folder. If we have python workers working in parallel, that should be done by a subagent so we don't pollute the process orchestrators context window. Those workers should store their log entries in a SQLite db so we don't have concurrency and race conditions. We should be able to use a flag for capturing trace files for scripts that manage multiple prompts and responses, such as agentic web search. Business processes are run multiple times, so we should separate the process functional files from the process output files. The output files should be organized so each run is separated into its own folders. When we use scripts to run prompts, those prompts should be isolated for easy prompt refinement without having to touch the python script that is running it. We should not use the anthropic API on scripts we'll run locally, instead we use import the Claude Agent SDK so we don't incur API charges that should be covered by our Claude subscription. We should make sure projects can be run on multiple machines, so we shouldn't hard code local file paths that are specific to a machine. We should use .env files for our secrets. If we're running into issues or bugs, we should document what we're trying and the results so we make solid progress and don't try things that didn't work.