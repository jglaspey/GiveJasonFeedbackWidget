# ABOUTME: Test plan for understanding how skill file size affects Claude's behavior
# ABOUTME: Based on claims from "I Was Wrong About Skills" community insight

feature:
  name: skill_size_impact
  description: Does skill file size affect activation time, token usage, and response quality?
  category: claude-code

# Source: https://github.com/mrgoonie/claudekit-skills
# Claim: 200-line limit isn't arbitrary - it's based on how much context an LLM can efficiently scan
source_claims:
  - "Skills over 200 lines cause 'context sludge' and slow activation"
  - "4.8x token efficiency improvement from refactoring large skills"
  - "Activation time: ~500ms → <100ms after reducing skill size"
  - "Relevant information ratio: ~10% → ~90% after restructuring"

learning_goals:
  - Does skill size affect invocation decision?
  - Does skill size affect response quality?
  - Is there a measurable activation time difference?
  - At what size does degradation start?

hypotheses:
  - id: h1
    statement: "Skills under 200 lines are processed more efficiently than large skills"
    metrics:
      - token_usage
      - response_completeness
      - time_to_first_response

  - id: h2
    statement: "Large skills cause Claude to skip or summarize parts of the content"
    metrics:
      - sections_referenced_in_response
      - accuracy_of_skill_following

  - id: h3
    statement: "There's a threshold size where skill effectiveness degrades"
    metrics:
      - success_rate_by_size_bucket

experiments:
  - id: size_50
    hypothesis_id: h1
    description: Test with 50-line skill (well under threshold)
    setup:
      skill_config:
        name: test-skill-size-50
        line_count: 50
        content: "Basic instructions with 5 steps"
    test_prompts:
      - "Use the test-skill-size-50 skill"
    measurements:
      - token_count_in_response
      - all_steps_mentioned

  - id: size_200
    hypothesis_id: h1
    description: Test with 200-line skill (at claimed threshold)
    setup:
      skill_config:
        name: test-skill-size-200
        line_count: 200
        content: "Detailed instructions with 20 steps"
    test_prompts:
      - "Use the test-skill-size-200 skill"
    measurements:
      - token_count_in_response
      - all_steps_mentioned

  - id: size_500
    hypothesis_id: h1
    description: Test with 500-line skill (over threshold)
    setup:
      skill_config:
        name: test-skill-size-500
        line_count: 500
        content: "Extensive instructions with 50 steps"
    test_prompts:
      - "Use the test-skill-size-500 skill"
    measurements:
      - token_count_in_response
      - steps_mentioned_vs_total

  - id: size_1000
    hypothesis_id: h1
    description: Test with 1000-line skill (well over threshold)
    setup:
      skill_config:
        name: test-skill-size-1000
        line_count: 1000
        content: "Massive instructions with 100 steps"
    test_prompts:
      - "Use the test-skill-size-1000 skill"
    measurements:
      - token_count_in_response
      - steps_mentioned_vs_total
      - any_content_skipped

success_metrics:
  - "Clear correlation between size and effectiveness"
  - "Identified threshold where degradation begins"
  - "Quantified token efficiency difference"

testing_process:
  1_setup: |
    Create 4 test skills of varying sizes in tester_workspace/.claude/skills/
    Each should have numbered steps so we can verify completeness

    Example structure for each:
    - Frontmatter with name/description
    - Numbered instructions (Step 1, Step 2, etc.)
    - Each step should be verifiable in the response

  2_test: |
    For each skill size:
    python run_experiment.py "Use the test-skill-size-X skill and list all the steps"

    Capture:
    - Total tokens used (from ResultMessage.usage)
    - Time to complete (from ResultMessage.duration_ms)
    - Whether all steps were mentioned

  3_analyze: |
    Compare across sizes:
    - Token efficiency (tokens per step mentioned)
    - Completion rate (steps mentioned / total steps)
    - Response time

  4_record: |
    python record_experiment.py \
      --feature skill_size_impact \
      --hypothesis "..." \
      --prompt "..." \
      --expected ... \
      --actual ... \
      --notes "Size: X, Steps mentioned: Y/Z, Tokens: N"
