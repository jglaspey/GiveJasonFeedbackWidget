# ABOUTME: Test plan for validating externalized memory pattern with Agent SDK
# ABOUTME: Based on claims from "Long Running Agents" community insight

feature:
  name: externalized_memory
  description: Does the Agent SDK maintain state across calls via project files?
  category: agent-sdk

# Source: "Long Running Agents" pattern
# Core claim: "The LLM is stateless, but the project harness makes the system stateful"
source_claims:
  - "Project files ARE the memory"
  - "Agent reads [files] every time - can stop/start days later"
  - "Memory externalized in files: progress logs, task lists, commit history"
  - "Every session reads these artifacts back in - never forgets where it left off"
  - "Deterministic cycle: load state → pick task → implement → update state → commit"

learning_goals:
  - Can sequential Agent SDK calls share state via files?
  - Does the agent correctly read and interpret progress files?
  - Is there continuity between sessions?
  - What file formats work best for state persistence?

hypotheses:
  - id: h1
    statement: "Agent reads state files at the start of each session"
    evidence_needed:
      - Read tool calls for state files
      - Agent references content from state files in response

  - id: h2
    statement: "Agent can continue work based on externalized state"
    evidence_needed:
      - First session creates state
      - Second session reads and continues correctly

  - id: h3
    statement: "Structured formats (JSON, YAML) work better than prose"
    evidence_needed:
      - Compare accuracy with JSON vs prose progress logs
      - Measure task identification accuracy

  - id: h4
    statement: "Agent correctly identifies 'next task' from task list"
    evidence_needed:
      - Agent picks correct incomplete task
      - Doesn't repeat completed tasks

experiments:
  - id: state_file_reading
    hypothesis_id: h1
    description: Verify agent reads state files when prompted
    setup:
      files:
        - progress.log: |
            # Progress Log
            2025-12-01 10:00: Started project
            2025-12-01 10:30: Completed task 1 - Setup
            2025-12-01 11:00: Started task 2 - Implementation
        - tasks.json: |
            {
              "tasks": [
                {"id": 1, "name": "Setup", "status": "completed"},
                {"id": 2, "name": "Implementation", "status": "in_progress"},
                {"id": 3, "name": "Testing", "status": "pending"}
              ]
            }
    test_prompts:
      - "What's the current status of this project?"
    expected:
      - Agent reads progress.log and/or tasks.json
      - Agent correctly reports task 2 is in progress

  - id: cross_session_continuity
    hypothesis_id: h2
    description: Test if second session continues from first
    setup:
      session_1:
        prompt: "Create a tasks.json with 3 tasks, mark task 1 as done"
        expected_output: tasks.json with task 1 completed
      session_2:
        prompt: "Continue working on this project. What's the next task?"
        expected: Agent identifies task 2 as next
    observe:
      - Does session 2 correctly identify task 2?
      - Does it reference task 1 as already completed?

  - id: json_vs_prose
    hypothesis_id: h3
    description: Compare structured vs unstructured state files
    setup:
      structured:
        tasks.json: '{"tasks": [{"id": 1, "done": true}, {"id": 2, "done": false}]}'
      unstructured:
        tasks.txt: "Task 1 is done. Task 2 is not done yet."
    test_prompts:
      - "What task should I work on next?"
    measurements:
      - accuracy_structured
      - accuracy_unstructured
      - confidence_of_response

  - id: task_selection_accuracy
    hypothesis_id: h4
    description: Test if agent correctly picks next incomplete task
    setup:
      tasks.json: |
        {
          "tasks": [
            {"id": 1, "name": "Setup database", "status": "completed"},
            {"id": 2, "name": "Create API", "status": "completed"},
            {"id": 3, "name": "Build UI", "status": "pending"},
            {"id": 4, "name": "Write tests", "status": "pending"},
            {"id": 5, "name": "Deploy", "status": "pending"}
          ]
        }
    test_prompts:
      - "Pick exactly ONE task to work on next"
    expected:
      - Agent picks task 3 (first pending)
      - Agent does NOT pick completed tasks
      - Agent picks only ONE task

  - id: state_update_persistence
    hypothesis_id: h2
    description: Verify agent updates and state persists
    setup:
      initial_state:
        tasks.json: '{"current_task": null, "completed": []}'
    session_1:
      prompt: "Mark 'setup' as the current task and save to tasks.json"
      verify: tasks.json updated with current_task = "setup"
    session_2:
      prompt: "What task am I working on?"
      expected: Agent reads tasks.json, reports "setup"

success_metrics:
  - "Demonstrated cross-session state persistence"
  - "Quantified accuracy of task identification"
  - "Compared structured vs unstructured formats"
  - "Validated the long-running agent pattern"

testing_process:
  1_setup: |
    Create state files in tester_workspace/

    Files to create:
    - tasks.json (structured task list)
    - progress.log (prose progress log)
    - project.md (project definition)

  2_test_single_session: |
    python run_experiment.py "What's the project status?"

    Verify:
    - Agent reads state files (check tool_calls)
    - Agent correctly interprets state

  3_test_cross_session: |
    # Session 1: Create state
    python run_experiment.py "Create a task list with 3 items, mark first as done"

    # Verify file was created/updated

    # Session 2: Read state
    python run_experiment.py "What's the next task to work on?"

    # Verify continuity

  4_record: |
    python record_experiment.py \
      --feature externalized_memory \
      --hypothesis "..." \
      --prompt "..." \
      --expected ... \
      --actual ... \
      --notes "Files read: [list], State correctly interpreted: yes/no"

implications: |
  If externalized memory works well, this validates:
  1. Our learning framework's use of knowledge_base.db
  2. The sequential-processing skill's progress.jsonl pattern
  3. The long-running agent architecture from the community insight

  If it doesn't work well, we need:
  1. Better file format conventions
  2. More explicit prompting about state
  3. Possibly hooks to inject state automatically
