# ABOUTME: Test plan for advanced Agent SDK features and patterns
# ABOUTME: Based on gaps in agent-sdk-basics skill and original vision

feature:
  name: agent_sdk_advanced
  description: Advanced Agent SDK patterns beyond basics - sessions, tools, hooks integration
  category: agent-sdk

# From agent-sdk-basics skill - documented but needs validation:
# - ClaudeAgentOptions fields
# - Permission modes
# - Message types
# Questions that remain from original implementation:
remaining_questions:
  - "Does system_prompt override CLAUDE.md?"
  - "What's the exact message type structure?"
  - "How do hooks fire in SDK context?"
  - "Can we register custom tools via SDK?"
  - "What's the performance difference between query() and ClaudeSDKClient?"

# From LEARNINGS.md discoveries:
validated_knowledge:
  - add_dirs enables skill discovery
  - ClaudeAgentOptions fields match documented
  - Message types are dataclasses not dicts
  - Hooks fire in Agent SDK context

learning_goals:
  - Validate all ClaudeAgentOptions fields work as expected
  - Understand system_prompt vs CLAUDE.md interaction
  - Test all permission_mode values
  - Document complete message type structures
  - Test hooks in SDK-spawned sessions
  - Understand session lifecycle management

hypotheses:
  - id: h1
    statement: "system_prompt in options overrides CLAUDE.md"
    evidence_needed:
      - Test with both present
      - Which takes precedence?
      - Can they be combined?

  - id: h2
    statement: "permission_mode 'bypassPermissions' allows all operations"
    evidence_needed:
      - Test with restrictive settings.json
      - bypassPermissions should ignore restrictions
      - Document security implications

  - id: h3
    statement: "max_turns prevents infinite loops in agent conversations"
    evidence_needed:
      - Set low max_turns
      - Trigger tool loop
      - Verify stops at limit

  - id: h4
    statement: "settings parameter overrides discovered settings"
    evidence_needed:
      - Have settings.json in workspace
      - Pass different settings via parameter
      - Verify parameter wins

  - id: h5
    statement: "Hooks fire correctly in SDK-spawned sessions"
    evidence_needed:
      - Configure hooks in settings.json
      - Spawn via Agent SDK
      - Verify hooks execute

  - id: h6
    statement: "Session state persists within ClaudeSDKClient context"
    evidence_needed:
      - Send multiple messages in same session
      - Verify context maintained
      - Compare to query() stateless behavior

experiments:
  - id: system_prompt_override
    hypothesis_id: h1
    description: Test system_prompt vs CLAUDE.md interaction
    setup:
      claude_md: "You are a helpful assistant focused on Python."
      system_prompt: "You are an expert in JavaScript only."
    test_prompts:
      - "What programming language do you specialize in?"
    expected_outcomes:
      - If system_prompt wins: "JavaScript"
      - If CLAUDE.md wins: "Python"
      - If combined: Both mentioned

  - id: permission_modes
    hypothesis_id: h2
    description: Test all permission_mode values
    test_cases:
      - mode: "default"
        action: Write file
        expected: Prompts user

      - mode: "acceptEdits"
        action: Write file
        expected: Auto-approved

      - mode: "plan"
        action: Any tool
        expected: "Planning mode behavior?"

      - mode: "bypassPermissions"
        action: Restricted operation
        expected: Auto-approved (bypass settings.json)

  - id: max_turns_limit
    hypothesis_id: h3
    description: Test max_turns prevents infinite loops
    setup:
      max_turns: 3
    test_prompts:
      - "Keep running tool calls until I tell you to stop"
    expected:
      - Stops after 3 turns
      - Graceful termination
      - Error or warning message?

  - id: settings_parameter
    hypothesis_id: h4
    description: Test settings parameter override
    setup:
      settings_json: '{"allowed_tools": ["Read(*)"]}'
      parameter_settings: '{"allowed_tools": ["*"]}'
    test:
      - Spawn with parameter_settings
      - Try Write operation
      - Should be allowed (parameter wins)

  - id: hooks_in_sdk
    hypothesis_id: h5
    description: Test hooks fire in SDK sessions
    setup:
      settings_json: |
        {
          "hooks": {
            "UserPromptSubmit": [{
              "hooks": [{"type": "command", "command": "echo 'HOOK_FIRED' >> /tmp/sdk_hook_test"}]
            }]
          }
        }
    test:
      - Clear /tmp/sdk_hook_test
      - Spawn SDK session
      - Send prompt
      - Check if hook fired
    expected:
      - Hook executes in SDK context
      - File contains HOOK_FIRED

  - id: session_state
    hypothesis_id: h6
    description: Test session state persistence
    test:
      session_test: |
        async with ClaudeSDKClient() as client:
            await client.send("Remember the number 42")
            response = await client.send("What number did I ask you to remember?")
            # Should remember 42

      query_test: |
        await query("Remember the number 42")
        response = await query("What number did I ask you to remember?")
        # Should NOT remember (stateless)

success_metrics:
  - "All ClaudeAgentOptions fields tested"
  - "Permission modes documented with examples"
  - "Session state behavior clarified"
  - "Hooks in SDK context validated"
  - "Best practices for advanced usage"

testing_process:
  1_options_exploration: |
    Systematically test each ClaudeAgentOptions field:

    python -c "
    from claude_agent_sdk import ClaudeAgentOptions
    import dataclasses

    for field in dataclasses.fields(ClaudeAgentOptions):
        print(f'{field.name}: {field.type}')
    "

  2_permission_mode_tests: |
    For each permission mode:
    - Create session with that mode
    - Try various operations
    - Document behavior

  3_hooks_test: |
    Configure hooks in tester_workspace/settings.json
    Spawn via run_experiment.py
    Verify hooks fire

  4_session_comparison: |
    Compare query() vs ClaudeSDKClient:
    - Context retention
    - Performance
    - Use cases

notes: |
  This builds on the agent-sdk-basics skill with empirical validation.

  The skill documents patterns, this test plan validates them.

  Key questions from original vision still open:
  1. What's the best pattern for long-running processes?
  2. How to handle session timeouts?
  3. Can we catch and handle SDK errors gracefully?
  4. What's the token/cost model for SDK usage?

  Integration considerations:
  - How does SDK interact with hooks?
  - How does SDK interact with skills?
  - How does SDK interact with subagents?
