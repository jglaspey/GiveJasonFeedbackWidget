# ABOUTME: Test plan for creating a skill-creation skill (meta-skill)
# ABOUTME: Synthesizes Anthropic best practices, Jesse Vincent insights, and empirical learnings

feature:
  name: skill_creation
  description: Patterns and processes for creating effective Claude Code skills
  category: skills

# Sources synthesized:
# 1. Anthropic's official skill-creator SKILL.md
# 2. claude_documentation/claude_skills/agent_skills_best_practices.md
# 3. claude_documentation/claude_skills/agent_skills_overview.md
# 4. Jesse Vincent's blog: skills-for-claude
# 5. knowledge_base.db empirical learnings

# Key insight from Jesse Vincent:
# "Separating 'When should Claude use this skill?' from 'What does this skill do?'
#  leads to better compliance. When Claude thinks it knows what a skill does,
#  it's more likely to believe it's using the skill and just wing it."

learning_goals:
  - Validate progressive disclosure pattern reduces token usage
  - Test "when to use" vs "what to do" separation improves compliance
  - Confirm distinctive keywords in descriptions trigger invocation
  - Validate skill size thresholds (empirical vs community advice)
  - Test TDD approach for skill development
  - Document degrees of freedom tradeoffs

# From knowledge_base.db validated patterns:
validated_from_knowledge_base:
  - pattern: "Explicit skill name always works"
    confidence: 0.95
    implication: "Always test with explicit invocation first"

  - pattern: "Distinctive keywords in description help"
    confidence: 0.70
    implication: "Use domain-specific terms, not generic verbs"

  - pattern: "Claude follows explicit reference instructions"
    confidence: 0.90
    implication: "Progressive disclosure works via explicit file mapping"

  - pattern: "200-line threshold is not a hard limit"
    confidence: 0.85
    implication: "Community advice may be outdated - test empirically"

  - gotcha: "Generic verbs don't trigger skills"
    severity: medium
    implication: "Avoid review, analyze, check in descriptions"

hypotheses:
  - id: h1_progressive_disclosure
    statement: "Three-tier progressive disclosure reduces token usage without hurting effectiveness"
    source: "Anthropic best practices + knowledge_base pattern"
    evidence_needed:
      - Measure tokens used with flat skill vs tiered skill
      - Verify Claude only loads relevant references
      - Test that Claude can still accomplish same tasks

  - id: h2_when_vs_what
    statement: "Separating 'when to use' (description) from 'what to do' (body) improves compliance"
    source: "Jesse Vincent insight"
    evidence_needed:
      - Create skill with mixed description
      - Create skill with separated concerns
      - Compare invocation rates on same prompts
      - Track "winging it" behavior (Claude acting without reading skill)

  - id: h3_distinctive_keywords
    statement: "Domain-specific keywords in description improve implicit invocation"
    source: "knowledge_base.db pattern (0.70 confidence)"
    evidence_needed:
      - Test generic description ("helps with code review")
      - Test specific description ("identifies anomalies in billing reconciliation")
      - Compare invocation rates without explicit skill name

  - id: h4_skill_size
    statement: "Skills up to 500 lines work fine on Opus 4.5 despite 200-line community advice"
    source: "knowledge_base.db pattern (0.85 confidence)"
    evidence_needed:
      - Test 100-line skill
      - Test 300-line skill
      - Test 500-line skill
      - Measure comprehension and step completion

  - id: h5_tdd_skills
    statement: "Test-Driven Development for skills catches compliance gaps early"
    source: "Jesse Vincent approach"
    evidence_needed:
      - Create test prompts before skill
      - Measure initial compliance
      - Iterate on skill based on failures
      - Track improvement over iterations

  - id: h6_degrees_of_freedom
    statement: "Lower degrees of freedom (exact scripts) have higher consistency but lower adaptability"
    source: "Anthropic best practices"
    evidence_needed:
      - High freedom: text instructions only
      - Medium freedom: pseudocode with params
      - Low freedom: exact scripts in scripts/
      - Compare consistency vs adaptability across contexts

  - id: h7_explicit_topic_mapping
    statement: "Explicit 'for X, read references/X.md' instructions work reliably"
    source: "knowledge_base.db pattern (0.90 confidence)"
    evidence_needed:
      - Skill with implicit organization
      - Skill with explicit topic-to-file mapping
      - Track which reference files Claude reads

experiments:
  - id: progressive_disclosure_tokens
    hypothesis_id: h1_progressive_disclosure
    description: Measure token impact of progressive disclosure
    setup:
      flat_skill: |
        # All content in SKILL.md
        # ~400 lines covering all topics

      tiered_skill: |
        # SKILL.md: ~100 lines entry point
        # references/topic1.md, references/topic2.md, etc.
        # Explicit: "For topic1, read references/topic1.md"
    test_prompts:
      - "Help me with topic1"  # Should only load topic1 reference
      - "Help me with topic2"  # Should only load topic2 reference
      - "Help me with everything"  # May load more
    metrics:
      - Tokens used per prompt
      - Task completion success
      - Reference files accessed

  - id: when_vs_what_separation
    hypothesis_id: h2_when_vs_what
    description: Test Jesse Vincent's insight on separation
    setup:
      mixed_skill: |
        ---
        name: data-processor
        description: Processes CSV files by reading them, validating schema, transforming columns, and outputting JSON
        ---
        # Processing Data Files
        This skill processes CSV files...

      separated_skill: |
        ---
        name: data-processor
        description: When you need to convert CSV data or validate tabular file formats
        ---
        # Processing Data Files
        ## What This Skill Does
        1. Reads CSV files
        2. Validates schema
        3. Transforms columns
        4. Outputs JSON
    test_prompts:
      - "I have a CSV file that needs to become JSON"
      - "Can you process this data file?"
      - "Transform my spreadsheet data"
    observe:
      - Which skill version gets invoked more
      - Does Claude "wing it" with mixed description
      - Does Claude properly read body with separated version

  - id: keyword_distinctiveness
    hypothesis_id: h3_distinctive_keywords
    description: Test impact of distinctive vs generic keywords
    test_cases:
      - description: "helps analyze and review code changes"
        keywords: [analyze, review, code]
        expected: Low implicit invocation

      - description: "identifies anomalies in billing reconciliation workflows"
        keywords: [anomalies, billing, reconciliation]
        expected: Higher implicit invocation

      - description: "validates HIPAA compliance in patient data exports"
        keywords: [HIPAA, compliance, patient]
        expected: Higher implicit invocation
    test_prompts:
      - "Look at these billing numbers"  # Should trigger distinctive
      - "Review this code"  # Generic - may not trigger
      - "Check the patient export for issues"  # Should trigger specific

  - id: skill_size_empirical
    hypothesis_id: h4_skill_size
    description: Empirically test skill size thresholds
    test_skills:
      - name: small_skill
        lines: 100
        sections: 3

      - name: medium_skill
        lines: 300
        sections: 10

      - name: large_skill
        lines: 500
        sections: 15
    tests_per_skill:
      - "Complete all steps from section 1"
      - "Follow the complete workflow"
      - "Apply all guidelines from sections 5-10"
    metrics:
      - Steps completed correctly
      - Steps skipped or improvised
      - References to actual skill content vs hallucinated

  - id: tdd_skill_iteration
    hypothesis_id: h5_tdd_skills
    description: Test TDD approach for skill development
    process:
      1_write_tests_first: |
        Before writing skill, create test prompts:
        - "Create a new skill for X"
        - "What files should a skill have?"
        - "How should I structure the skill?"

      2_measure_baseline: |
        Run test prompts with NO skill
        Record Claude's default behavior
        Identify gaps from desired behavior

      3_create_minimal_skill: |
        Write skill targeting identified gaps
        Focus on "when to use" clarity

      4_test_and_iterate: |
        Run same test prompts
        Compare to baseline
        Find remaining gaps
        Iterate until compliance reaches target
    metrics:
      - Baseline compliance rate
      - Post-skill compliance rate
      - Iterations to reach 90% compliance
      - Types of failures (didn't invoke, invoked but ignored, etc.)

  - id: degrees_of_freedom_tradeoff
    hypothesis_id: h6_degrees_of_freedom
    description: Test consistency vs adaptability tradeoff
    skill_variants:
      high_freedom: |
        ## Creating Skills
        Write clear descriptions. Use progressive disclosure.
        Structure files logically. Test with real prompts.

      medium_freedom: |
        ## Creating Skills
        1. Create directory: skills/{skill-name}/
        2. Write SKILL.md with:
           - YAML frontmatter (name, description)
           - Body with instructions
        3. Add references/ for detailed docs
        4. Add scripts/ for executable code

      low_freedom: |
        ## Creating Skills
        Run: python scripts/init_skill.py {name}
        Edit: skills/{name}/SKILL.md
        Test: python scripts/test_skill.py {name}
    test_prompts:
      - "Create a skill for handling API errors"  # Standard case
      - "Create a skill for our custom deployment process"  # Novel case
      - "Update the existing git skill"  # Modification case
    metrics:
      - Consistency (same output for same prompt)
      - Adaptability (handles novel cases)
      - User satisfaction (did it meet needs?)

  - id: explicit_reference_mapping
    hypothesis_id: h7_explicit_topic_mapping
    description: Test explicit vs implicit reference organization
    setup:
      implicit_skill: |
        # Skill Creation Guide
        See references/ for detailed documentation.

        references/
        ├── structure.md
        ├── naming.md
        ├── testing.md
        └── iteration.md

      explicit_skill: |
        # Skill Creation Guide

        **For structure questions**: Read `references/structure.md`
        **For naming conventions**: Read `references/naming.md`
        **For testing approach**: Read `references/testing.md`
        **For iteration patterns**: Read `references/iteration.md`
    test_prompts:
      - "How should I name my skill?"
      - "What's the testing approach?"
      - "How do I structure the files?"
    observe:
      - Which reference file Claude reads
      - Does Claude read unnecessary files
      - Response accuracy

success_metrics:
  - "Progressive disclosure validated with token measurements"
  - "When vs what separation effect quantified"
  - "Keyword distinctiveness guidelines established"
  - "Skill size thresholds validated for current model"
  - "TDD workflow documented with iteration counts"
  - "Degrees of freedom tradeoffs documented"
  - "Explicit reference mapping confirmed effective"

# Anthropic's 6-step skill creation process to validate:
anthropic_process:
  step_1_understand:
    description: "Understand the skill deeply with concrete examples"
    validation_needed:
      - Does gathering examples first improve skill quality?
      - How many examples are sufficient?

  step_2_plan:
    description: "Plan what content can be reused across applications"
    validation_needed:
      - Does planning reusability reduce duplication?
      - What belongs in SKILL.md vs references vs scripts?

  step_3_initialize:
    description: "Initialize skill structure (manual or with init_skill.py)"
    validation_needed:
      - Does using init script improve consistency?
      - What's the minimum viable structure?

  step_4_edit:
    description: "Edit SKILL.md and add resources"
    validation_needed:
      - What makes a good SKILL.md?
      - When should content go in references vs inline?

  step_5_package:
    description: "Package for distribution (if needed)"
    validation_needed:
      - Is packaging necessary for local skills?
      - What metadata matters?

  step_6_iterate:
    description: "Iterate based on real usage"
    validation_needed:
      - How to capture feedback effectively?
      - What signals indicate iteration is needed?

# Jesse Vincent's TDD approach to validate:
jesse_vincent_approach:
  key_insight: |
    "In my testing, I've found that showing only 'When should Claude use this skill?'
    leads to better compliance. When Claude thinks it knows what a skill does, it's
    more likely to believe it's using the skill and just wing it."

  tdd_process:
    1_write_failing_tests: "Create prompts that should use the skill"
    2_measure_baseline: "See how Claude handles without the skill"
    3_create_skill: "Target the gap between desired and actual"
    4_test_compliance: "Does Claude use the skill correctly?"
    5_find_rationalizations: "Why does Claude skip the skill?"
    6_iterate: "Adjust description and instructions"

  validation_needed:
    - Does TDD catch issues faster than write-then-test?
    - What are common Claude rationalizations for skipping skills?
    - How to make descriptions that prevent "winging it"?

# Best practices to incorporate in final skill:
best_practices_checklist:
  from_anthropic:
    - "Keep SKILL.md body under 500 lines (optimal)"
    - "Keep references one level deep"
    - "Use third-person descriptions ('Processes X' not 'Process X')"
    - "Use gerund naming ('Processing PDFs' not 'PDF Processor')"
    - "Include feedback loops: run → validate → fix → repeat"
    - "Test with multiple models (Opus, Sonnet, Haiku)"
    - "Maximum 64 chars for name, 1024 for description"

  from_knowledge_base:
    - "Use distinctive domain-specific terms in descriptions"
    - "Avoid generic verbs: review, analyze, check"
    - "Use explicit topic-to-file mapping for references"
    - "Test empirically - community advice may be outdated"

  from_jesse_vincent:
    - "Separate 'when to use' from 'what to do'"
    - "Use TDD approach for skill development"
    - "Find Claude's rationalizations for skipping"
    - "Skills are prompt injection by design - be security aware"

testing_process:
  1_setup: |
    Create test workspace with:
    - Baseline skills (no optimization)
    - Optimized skills (with all best practices)
    - Test prompt suites for each hypothesis

  2_baseline_measurement: |
    For each skill type:
    - Run test prompts
    - Measure invocation rate
    - Measure task completion
    - Measure token usage

  3_progressive_experiments: |
    Test hypotheses in order:
    - H1: Progressive disclosure (foundational)
    - H2: When vs what separation
    - H3: Keyword distinctiveness
    - H4: Skill size validation
    - H5: TDD effectiveness
    - H6: Degrees of freedom
    - H7: Explicit mapping

  4_synthesis: |
    Combine findings into:
    - Validated best practices
    - Empirical thresholds
    - Creation workflow
    - The actual skill-creation skill

notes: |
  This test plan is comprehensive because skill creation is foundational.
  Every other skill we create benefits from getting this right.

  Key tensions to resolve:
  1. Conciseness vs completeness (progressive disclosure helps)
  2. Flexibility vs consistency (degrees of freedom)
  3. Implicit vs explicit invocation (keyword distinctiveness)
  4. Generic vs specific (domain terms)

  The final skill should embody all validated best practices.
  It should be self-demonstrating - a skill for creating skills that
  exemplifies all the patterns it teaches.

  Security note from Jesse Vincent:
  "Skills are prompt injection by design" - any executable scripts
  in skills/ can run arbitrary code. This is power, not a bug,
  but the skill-creation skill should document this clearly.
