# ABOUTME: Overview test plan for skill enforcement via hooks
# ABOUTME: Validates whether hooks can reliably enforce skill usage at the right moments

feature:
  name: skill_enforcement
  description: Can hooks enforce skill usage based on context detection?
  category: claude-code

# The core problem we're solving
problem_statement: |
  Skills exist but Claude often skips them, rationalizing "I know this already" or
  "this is too simple". The skill-discipline skill tries to solve this via prompting,
  but prompts can be ignored.

  Hypothesis: Hooks can provide hard enforcement by detecting context and surfacing
  relevant skills at the moment they're needed.

# Skills categorized by enforcement approach
skill_categories:
  workflow_skills:
    description: "Invoked at specific workflow moments - use slash commands"
    skills:
      - project-discovery  # /new-project
      - writing-plans      # /plan (proposed)
      - executing-plans    # /execute-plan (proposed)
      - code-review        # /review or PR trigger (proposed)
    enforcement: "Slash commands - explicit invocation, not hooks"
    test_plan: "N/A - not part of this validation"

  implementation_discipline:
    description: "Easy to skip when coding - need enforcement"
    skills:
      - test-driven-development
      - verification-before-completion
      - systematic-debugging
      - testing-anti-patterns
      - condition-based-waiting
      - defense-in-depth
    enforcement: "PreToolUse hooks detecting file patterns and content"
    test_plan: "Core focus of validation"

  reference_skills:
    description: "Needed when working on specific file types"
    skills:
      - writing-skills         # When editing .claude/skills/**
      - git-operations         # When running git commands (already enforced)
      - sequential-processing  # When writing processing code
      - parallel-processing    # When writing worker code
      - settings-json-patterns # When editing settings.json
    enforcement: "PreToolUse hooks detecting file paths"
    test_plan: "Secondary focus"

  setup_skills:
    description: "One-time setup, pure reference"
    skills:
      - directory-structure
      - path-management
      - prompt-isolation
      - agent-project-setup
      - agent-sdk-basics
      - railway-agent-sdk-deployment
      - git-worktrees
    enforcement: "None needed - reference material"
    test_plan: "N/A"

# Key questions to validate
learning_goals:
  - Can UserPromptSubmit hooks reliably detect debugging context?
  - Can PreToolUse hooks detect file types being edited?
  - Can PreToolUse hooks analyze file content for anti-patterns?
  - Do enforcement hooks add unacceptable latency?
  - Do warnings get ignored? Is blocking necessary?
  - Can we track session state (e.g., "test written before production code")?

# Sub-test plans in this directory
sub_plans:
  - 01_prompt_context_detection.yaml    # UserPromptSubmit pattern matching
  - 02_file_path_enforcement.yaml       # PreToolUse file path triggers
  - 03_content_pattern_detection.yaml   # PreToolUse content analysis
  - 04_tdd_enforcement.yaml             # Session state for TDD
  - 05_warning_vs_blocking.yaml         # Soft vs hard enforcement
  - 06_latency_impact.yaml              # Performance testing

success_criteria:
  - "Enforcement triggers reliably (>90% precision)"
  - "False positives are rare (<10%)"
  - "Latency acceptable (<500ms per hook)"
  - "Claude actually follows enforced skills"
  - "skill-discipline can be deprecated"
