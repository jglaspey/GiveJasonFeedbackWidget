# ABOUTME: Test plan for measuring latency impact of enforcement hooks
# ABOUTME: Validates whether hooks are fast enough for practical use

feature:
  name: latency_impact
  description: Do enforcement hooks add unacceptable latency?
  category: skill_enforcement

core_question: |
  Every hook execution adds latency to tool calls. With multiple enforcement
  hooks (prompt detection, file path, content analysis, TDD state), does the
  cumulative latency become noticeable and annoying?

# Latency budgets
latency_targets:
  individual_hook: "< 100ms"
  cumulative_per_tool: "< 300ms"
  user_perception: "< 500ms (feels instant)"
  unacceptable: "> 1000ms (noticeable delay)"

hypotheses:
  - id: h1
    statement: "Python hooks add ~50-100ms startup overhead"
    rationale: "Python interpreter startup is not instant"
    evidence_needed:
      - Baseline: empty Python hook latency
      - Compare to bash hook latency

  - id: h2
    statement: "File I/O (state files) adds ~10-50ms"
    evidence_needed:
      - Measure with/without state file reads
      - Measure with/without state file writes

  - id: h3
    statement: "Content analysis adds ~50-100ms for typical files"
    evidence_needed:
      - Regex pattern matching time
      - Varies with file size

  - id: h4
    statement: "Cumulative latency stays under 300ms with all hooks"
    evidence_needed:
      - Measure with all proposed hooks active
      - Measure across different tool types

experiments:
  - id: baseline_python_hook
    hypothesis_id: h1
    description: Measure empty Python hook overhead
    setup:
      hook_script: |
        #!/usr/bin/env python3
        import json
        import sys
        json.load(sys.stdin)
        sys.exit(0)
    measure:
      - "Time from hook start to hook exit"
      - "Run 100 times, calculate mean, p95, max"
    expected:
      mean: "< 50ms"
      p95: "< 100ms"

  - id: baseline_bash_hook
    hypothesis_id: h1
    description: Measure empty bash hook for comparison
    setup:
      hook_script: |
        #!/bin/bash
        cat > /dev/null
        exit 0
    measure:
      - "Time from hook start to hook exit"
      - "Run 100 times"
    expected:
      mean: "< 20ms"
      p95: "< 50ms"

  - id: state_file_overhead
    hypothesis_id: h2
    description: Measure state file read/write overhead
    setup:
      hook_variants:
        no_state: |
          #!/usr/bin/env python3
          import json, sys
          json.load(sys.stdin)
          sys.exit(0)

        read_only: |
          #!/usr/bin/env python3
          import json, sys
          from pathlib import Path
          data = json.load(sys.stdin)
          state_file = Path('/tmp/test_state.json')
          if state_file.exists():
              state = json.loads(state_file.read_text())
          sys.exit(0)

        read_write: |
          #!/usr/bin/env python3
          import json, sys
          from pathlib import Path
          data = json.load(sys.stdin)
          state_file = Path('/tmp/test_state.json')
          if state_file.exists():
              state = json.loads(state_file.read_text())
          else:
              state = {}
          state['count'] = state.get('count', 0) + 1
          state_file.write_text(json.dumps(state))
          sys.exit(0)
    measure:
      - "Compare latency across variants"
      - "Calculate I/O overhead"

  - id: content_analysis_overhead
    hypothesis_id: h3
    description: Measure regex pattern matching overhead
    setup:
      hook_script: |
        #!/usr/bin/env python3
        import json
        import sys
        import re
        import time

        start = time.time()

        data = json.load(sys.stdin)
        content = data.get('tool_input', {}).get('new_string', '')

        # Simulate all content patterns
        patterns = [
            r'time\.sleep\s*\(',
            r'await\s+asyncio\.sleep\s*\(',
            r'setTimeout\s*\(',
            r'expect\(.*mock.*\)\.toHaveBeenCalled',
            r'anthropic\.Anthropic\s*\(',
            r'from\s+anthropic\s+import',
        ]

        for p in patterns:
            re.search(p, content, re.IGNORECASE)

        elapsed = (time.time() - start) * 1000
        print(f"Content analysis: {elapsed:.1f}ms", file=sys.stderr)
    test_files:
      - size: "100 lines"
        expected: "< 10ms"
      - size: "1000 lines"
        expected: "< 50ms"
      - size: "10000 lines"
        expected: "< 200ms"

  - id: cumulative_all_hooks
    hypothesis_id: h4
    description: Measure with all proposed hooks active
    setup:
      hooks_active:
        UserPromptSubmit:
          - "prompt_context_detection (debugging/TDD keywords)"
        PreToolUse_Edit:
          - "file_path_enforcement"
          - "content_pattern_detection"
          - "tdd_state_tracking"
    measure:
      - "End-to-end latency for Edit tool call"
      - "End-to-end latency for Write tool call"
      - "Run 50 operations each"
    success_criteria:
      - "Mean < 200ms"
      - "P95 < 300ms"
      - "Max < 500ms"

  - id: real_world_perception
    hypothesis_id: h4
    description: Subjective perception of latency
    methodology: |
      Work normally for 1 day with all hooks active.
      Note any times latency feels noticeable.
    scale:
      1: "No noticeable delay"
      2: "Occasional slight delay"
      3: "Frequent noticeable delay"
      4: "Annoying delay"
      5: "Unacceptable delay"
    success_criteria:
      - "Score <= 2"

optimization_strategies:
  if_too_slow:
    - strategy: "Lazy loading"
      description: "Only import modules when needed"
      example: |
        # Instead of:
        import re
        import json
        from pathlib import Path

        # Do:
        import json
        import sys
        data = json.load(sys.stdin)
        if needs_regex:
            import re

    - strategy: "Bash for simple checks"
      description: "Use bash for path-only checks, Python for content"
      example: |
        # Bash hook for path check (fast):
        file_path=$(jq -r '.tool_input.file_path' < /dev/stdin)
        if [[ "$file_path" == *"/workers/"* ]]; then
            echo "Worker file detected"
        fi

    - strategy: "Parallel hook execution"
      description: "If Claude Code supports it, run independent hooks in parallel"

    - strategy: "Caching"
      description: "Cache compiled regex patterns between calls"
      note: "May not help if hook is fresh process each time"

    - strategy: "Skip large files"
      description: "Skip content analysis for files > 10KB"
      tradeoff: "Might miss anti-patterns in large files"

testing_process:
  1_baseline: |
    Measure current latency without enforcement hooks:
    - Time 50 Edit operations
    - Time 50 Write operations
    - Record mean, p95, max

  2_individual: |
    Add hooks one at a time, measure each:
    - Prompt detection hook
    - File path hook
    - Content analysis hook
    - TDD state hook

  3_cumulative: |
    Enable all hooks:
    - Measure same operations
    - Compare to baseline

  4_optimize: |
    If cumulative > 300ms:
    - Apply optimization strategies
    - Re-measure
    - Find acceptable configuration

  5_perception: |
    Work for 1 day with optimized hooks
    Rate subjective latency experience

success_metrics:
  - "Individual hooks < 100ms"
  - "Cumulative < 300ms"
  - "Subjective rating <= 2"
  - "Optimization path identified if needed"

notes: |
  Hook latency has compounding effects:
  - Each tool call runs all matching hooks
  - Complex operations have many tool calls
  - Perceived latency = sum of all hooks

  The git-operations hooks (existing) seem fast enough.
  Content analysis is the main risk for slowdown.

  Key question: Is the enforcement value worth the latency cost?
  If hooks save 5 minutes of debugging but add 1 second per edit,
  that's a good tradeoff for most sessions.
