# ABOUTME: Test plan for UserPromptSubmit hook detecting task context
# ABOUTME: Validates whether hooks can reliably identify debugging/bug-fix scenarios

feature:
  name: prompt_context_detection
  description: Can UserPromptSubmit hooks detect when specific skills apply?
  category: skill_enforcement

# The hypothesis
core_hypothesis: |
  A UserPromptSubmit hook can analyze the user's prompt and reliably detect
  when certain skills should be enforced, surfacing targeted reminders.

# Skills this applies to
target_skills:
  - systematic-debugging  # Triggered by error/bug language
  - test-driven-development  # Triggered by feature/fix requests

hypotheses:
  - id: h1
    statement: "Error-related keywords reliably indicate debugging context"
    keywords:
      high_confidence:
        - "error"
        - "bug"
        - "broken"
        - "failing"
        - "doesn't work"
        - "not working"
        - "crash"
        - "exception"
      medium_confidence:
        - "fix"
        - "issue"
        - "problem"
        - "wrong"
      low_confidence:
        - "check"
        - "look at"
        - "investigate"
    evidence_needed:
      - Keyword presence correlates with actual debugging task
      - False positive rate by keyword category

  - id: h2
    statement: "Feature/implementation keywords indicate TDD context"
    keywords:
      high_confidence:
        - "implement"
        - "add feature"
        - "create function"
        - "write code for"
        - "build"
      medium_confidence:
        - "add"
        - "create"
        - "make"
        - "new"
      context_modifiers:
        - "test" in prompt reduces TDD trigger (already testing)
        - "refactor" might need different handling
    evidence_needed:
      - Keyword presence correlates with code-writing task
      - Context modifiers improve accuracy

  - id: h3
    statement: "Combined keyword + context analysis improves accuracy"
    approach: |
      Instead of single keywords, look for patterns:
      - "X is broken" → debugging
      - "add X to Y" → implementation
      - "fix the X" → could be debugging OR implementation
    evidence_needed:
      - Pattern matching outperforms single keyword
      - Documented decision tree

experiments:
  - id: keyword_baseline
    hypothesis_id: h1
    description: Test each keyword against actual debugging scenarios
    setup:
      hook_config:
        type: UserPromptSubmit
        script: |
          #!/usr/bin/env python3
          import json
          import sys
          import re

          data = json.load(sys.stdin)
          prompt = data.get('user_prompt', '').lower()

          high_conf = ['error', 'bug', 'broken', 'failing', "doesn't work",
                       'not working', 'crash', 'exception']

          for kw in high_conf:
              if kw in prompt:
                  print(f"⚠️ DEBUGGING DETECTED: Use systematic-debugging skill")
                  print(f"   Trigger: '{kw}'")
                  break
    test_prompts:
      # True positives (should trigger)
      - prompt: "The tests are failing after my last change"
        expect: triggered
        is_debugging: true
      - prompt: "There's a bug in the login function"
        expect: triggered
        is_debugging: true
      - prompt: "The app crashes when I click submit"
        expect: triggered
        is_debugging: true
      - prompt: "This function doesn't work correctly"
        expect: triggered
        is_debugging: true

      # True negatives (should NOT trigger)
      - prompt: "Add a new login feature"
        expect: not_triggered
        is_debugging: false
      - prompt: "Refactor the user module"
        expect: not_triggered
        is_debugging: false
      - prompt: "What does this code do?"
        expect: not_triggered
        is_debugging: false

      # Edge cases
      - prompt: "Add error handling to this function"
        expect: uncertain  # Contains "error" but not debugging
        is_debugging: false
      - prompt: "Fix the typo in the comment"
        expect: uncertain  # Contains "fix" but trivial
        is_debugging: false

  - id: tdd_keyword_baseline
    hypothesis_id: h2
    description: Test TDD trigger keywords
    setup:
      hook_config:
        type: UserPromptSubmit
        script: |
          #!/usr/bin/env python3
          import json
          import sys

          data = json.load(sys.stdin)
          prompt = data.get('user_prompt', '').lower()

          # Skip if already about testing
          if 'test' in prompt and ('write test' in prompt or 'add test' in prompt):
              sys.exit(0)

          impl_keywords = ['implement', 'add feature', 'create function',
                          'write code', 'build']

          for kw in impl_keywords:
              if kw in prompt:
                  print(f"⚠️ IMPLEMENTATION DETECTED: Use test-driven-development skill")
                  print(f"   Write the failing test FIRST")
                  break
    test_prompts:
      # True positives
      - prompt: "Implement a retry mechanism"
        expect: triggered
        needs_tdd: true
      - prompt: "Add a feature to export data as CSV"
        expect: triggered
        needs_tdd: true
      - prompt: "Create a function to validate emails"
        expect: triggered
        needs_tdd: true

      # True negatives
      - prompt: "Write a test for the login function"
        expect: not_triggered
        needs_tdd: false  # Already writing test
      - prompt: "What does this function do?"
        expect: not_triggered
        needs_tdd: false
      - prompt: "Run the tests"
        expect: not_triggered
        needs_tdd: false

  - id: false_positive_rate
    hypothesis_id: h1
    description: Measure false positive rate across diverse prompts
    test_prompts:
      # Normal development prompts that shouldn't trigger debugging
      - "Read the config file"
      - "Show me the user model"
      - "What tests exist?"
      - "List the API endpoints"
      - "Add error handling"  # Edge case
      - "Handle the error case"  # Edge case
      - "Return an error message"  # Edge case
      - "Log errors to file"  # Edge case
    observe:
      - Count triggers vs non-triggers
      - Calculate false positive rate
    success_criteria:
      - "< 20% false positive rate on non-debugging prompts"

testing_process:
  1_setup: |
    Create hook script: .claude/hooks/detect-skill-context.py

    Register in settings.json:
    {
      "hooks": {
        "UserPromptSubmit": [{
          "hooks": [{
            "type": "command",
            "command": "$CLAUDE_PROJECT_DIR/.claude/hooks/detect-skill-context.py"
          }]
        }]
      }
    }

  2_test: |
    For each test prompt:
    1. Send prompt to Claude
    2. Record: Did hook trigger? What keyword?
    3. Record: Was it actually a debugging/implementation task?
    4. Calculate precision/recall

  3_record: |
    python record_experiment.py \
      --feature prompt_context_detection \
      --hypothesis "h1" \
      --prompt "..." \
      --expected triggered|not_triggered \
      --actual triggered|not_triggered \
      --notes "Keyword: X, Actually debugging: Y"

success_metrics:
  - "Precision > 80% (when triggered, usually correct)"
  - "Recall > 70% (catches most real debugging scenarios)"
  - "Clear keyword tiers established"
  - "Edge cases documented"

notes: |
  Key insight from earlier discussion: The current skill-discipline skill
  tries to do this via prompting ("Before responding, check: Does a skill apply?")
  but prompts can be ignored.

  This experiment tests whether hooks can provide harder enforcement.

  Risk: Over-triggering will cause me to ignore the warnings.
  We need high precision, even at cost of some recall.
